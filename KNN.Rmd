---
title: "KNN"
author: "Brian Wright"
date: "October 31, 2017"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#getwd()
setwd("/cloud/project/KNN")
```

```{r}
library(tidyverse)
```


```{r}
bank_data = read.csv("bank.csv", #<- name of the data set.
                     check.names = FALSE, #<- don't change column names.
                     stringsAsFactors = FALSE)#<- don't convert the numbers and characters to factors.

# Check the structure and view the data.
str(bank_data)

#Scale the features we will be using for classification 
bank_data[, c("age","duration","balance")] <- lapply(bank_data[, c("age","duration","balance")],function(x) scale(x))

str(bank_data)
```


```{r}
# Let's run the kNN algorithm on our banking data. 
# Check the composition of labels in the data set. 
table(bank_data$`signed up`)
table(bank_data$`signed up`)[2] / sum(table(bank_data$`signed up`))

# This means that at random, we have an 11.6% chance of correctly picking
# out a subscribed individual. Let's see if kNN can do any better.

# Let's split the data into a training and a test set.
# Sample 80% of our know data as training and 20% as test.
set.seed(1982)
bank_data_train_rows = sample(1:nrow(bank_data),#<- from 1 to the number of 
                                                     #rows in the data set
                              round(0.8 * nrow(bank_data), 0),  #<- multiply the number of rows by 0.8 and round the decimals
                              replace = FALSE)#<- don't replace the numbers

head(bank_data_train_rows)

# Let's check to make sure we have 80% of the rows. 
length(bank_data_train_rows) / nrow(bank_data)

bank_data_train = bank_data[bank_data_train_rows, ] #<- select the rows identified in the bank_data_train_rows data

                                                    
bank_data_test = bank_data[-bank_data_train_rows, ]  #<- select the rows that weren't identified in the bank_data_train_rows data

# Check the number of rows in each set.
nrow(bank_data_train)
nrow(bank_data_test)

```
Train the classifier 

```{r}
# Let's train the classifier for k = 3. 
# Install the "class" package that we'll use to run kNN.
# Take some time to learn about all its functionality.
#install.packages("class") 
library(class)
library(help = "class")  

# k-Nearest Neighbor is a randomized algorithm, so make sure to
# use set.seed() to make your results repeatable.
set.seed(1982)
bank_3NN <-  knn(train = bank_data_train[, c("age", "balance", "duration")],#<- training set cases
               test = bank_data_test[, c("age", "balance", "duration")],    #<- test set cases
               cl = bank_data_train[, "signed up"],#<- category for true classification
               k = 3,#<- number of neighbors considered
               use.all = TRUE,
               prob = TRUE) #<- control ties between class assignments If true, all distances equal to the kth largest are included

print(length(bank_data_test))
print(length(bank_data_train))


# View the output.
str(bank_3NN)
length(bank_3NN)
table(bank_3NN)
attributes(bank_3NN)

prb <- data.frame(prob=attr(bank_3NN, "prob"))
View(prb)
```
Compare to the original data

```{r}
# How does the kNN classification compare to the true class?
# Let's take a look at the confusion matrix by combining the 
# predictions from bank_3NN to the original data set.
kNN_res = table(bank_3NN,
                bank_data_test$`signed up`)
kNN_res

# Select the true positives and true negatives by selecting
# only the cells where the row and column names are the same.
kNN_res[row(kNN_res) == col(kNN_res)]

# Calculate the accuracy rate by dividing the correct classifications
# by the total number of classifications.
kNN_acc <-  sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)

kNN_sen <- kNN_res[2,2]/(kNN_res[2,2]+kNN_res[1,2])
kNN_sen

x <- (kNN_res[1,2])

kNN_acc

# An 87.0%ish accuracy rate is pretty good but keep in mind the base-rate is roughly 89/11, so we have more or less a 90% chance of guessing right if we don't know anything about the customer, but the negative outcomes we don't really care about, this models value is being able to id sign ups when they are actually sign ups. This requires us to know our true positive rate, or Sensitivity or Recall. (Ya, that's annoying.) So let's dig a little deeper.  

#Evaulation references: https://medium.com/ml-cheat-sheet/machine-learning-evaluation-metrics-b89b8832e275
# https://en.wikipedia.org/wiki/Precision_and_recall


library(caret)

confusionMatrix(as.factor(bank_3NN), as.factor(bank_data_test$`signed up`), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

#Use the medium reference above to calculate sensitivity 

#?confusionMatrix


#So our ability to "predict" sign up customers has more than doubled to 24% so that's good but still pretty bad overall. This means that out of 10 sign ups, we really only classify maybe 3 correctly. This is fairly typical when we have a unbalanced dataset. Which is why in this case we would want to tune this model on TPR (Sensitivity), to get it has high as possible while sacrificing Specificity or Precision.  Similar to a medical diagnosis example, where we would rather produce false positives as compared to false negatives, predict more of those with cancer that don't have it as compared to missing anyone that actually has cancer.      

#Reference for confusion matrix: https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/confusionMatrix 

#install.packages("scatterplot3d")
library(scatterplot3d)

attach(bank_data)

tdplot <- scatterplot3d(age, balance, duration, color="blue", pch = `signed up`)


```
Selecting the correct "k"
```{r}
# How does "k" affect classification accuracy? Let's create a function
# to calculate classification accuracy based on the number of "k."
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments#   If true, all distances equal to the kth largest are included
  conf_mat = table(class_knn, val_class)
  
  # Calculate the accuracy#could change this to Sensitivity 
  accu = sum(conf_mat[row(conf_mat) == col(conf_mat)]) / sum(conf_mat)                         
  cbind(k = k, accuracy = accu)
}


# The sapply() function plugs in several values into our chooseK function.
#sapply(x, fun...) "fun" here is passing a function to our k-function
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop! Returns a matrix.
knn_different_k = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = bank_data_train[, c("age", "balance", "duration")],
                                             val_set = bank_data_test[, c("age", "balance", "duration")],
                                             train_class = bank_data_train[, "signed up"],
                                             val_class = bank_data_test[, "signed up"]))



#A bit more of a explanation...
seq(1,21, by=2)#just creates a series of numbers
sapply(seq(1, 21, by=2), function(x) x+1)# sapply returns a new vector using the
# series of numbers and some calculation that is repeated over the vector of numbers 


# Reformatting the results to graph
str(knn_different_k)
class(knn_different_k)#matrix 
head(knn_different_k)

knn_different_k = tibble(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.

ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)

# 9 to 11 nearest neighbors seems to be a good choice because that's the
# greatest improvement in predictive accuracy before the incremental 
# improvement trails off.

```

```{r}
bank_5NN <-  knn(train = bank_data_train[, c("age", "balance", "duration")],
               test = bank_data_test[, c("age", "balance", "duration")],
               cl = bank_data_train[, "signed up"],
               k = 5,
               use.all = TRUE,
               prob = TRUE)


confusionMatrix(as.factor(bank_5NN), as.factor(bank_data_test$`signed up`), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

```

However, we care about Sensitivity right, so let's redo this with that in mind

```{r}
#Need to build a separate function
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments#   If true, all distances equal to the kth largest are included
  conf_mat = table(class_knn, val_class)
  
  # Calculate the accuracy#could change this to Sensitivity 
  sen = conf_mat[2,2]/(conf_mat[2,2]+conf_mat[1,2]) 

  cbind(k = k, sensitivity = sen)
}
```

```{r}
#Try it again...

knn_different_k = sapply(seq(1, 15, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = bank_data_train[, c("age", "balance", "duration")],
                                             val_set = bank_data_test[, c("age", "balance", "duration")],
                                             train_class = bank_data_train[, "signed up"],
                                             val_class = bank_data_test[, "signed up"]))


knn_different_k = tibble(k = knn_different_k[1,],
                             recall_sen = knn_different_k[2,])
View(knn_different_k)
# Plot accuracy vs. k.

ggplot(knn_different_k,
       aes(x = k, y = recall_sen)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)

#Wow that looks different...so what did we learn?

```

Example with Caret using 10-k cross-validation 

```{r}
library(caret)

set.seed(1981)

View(bank_data_train)

trctrl <- trainControl(method = "repeatedcv",
                       number = 10,
                       repeats = 3) # generic control to pass back into the knn mode using the cross validation method. 

train_bank_caret <- tibble(bank_data_train[, c("age", "balance", "duration","signed up")])

str(train_bank_caret)

train_bank_caret$`signed up` <- as.factor(train_bank_caret$`signed up`)

bank_knn_caret <- train(`signed up`~.,
                  data = train_bank_caret,
                  method="knn",
                  tuneLength=5,
                  trControl= trctrl,#cv method above, will select the optimal K
                  preProcess="scale") # already did this but helpful reference

bank_knn_caret # take a look

plot(bank_knn_caret)#can also plot

varImp(bank_knn_caret)#gives us variable importance on a range of 0 to 100

caret_pred <- predict(bank_knn_caret, bank_data_test)

caret_pred #gives a character predicted value for each row.

cf <- caret::confusionMatrix(as.factor(caret_pred), as.factor(bank_data_test$`signed up`)) 


cf$table

cf$byClass #Does this look right? 
```


Real quick - let's also look at a multi-class example using the iris dataset
```{r}

#For this example we are going to use the IRIS dataset in R
str(iris)
#first we want to scale the data so KNN will operate correctly
scalediris <- as.data.frame(scale(iris[1:4], center = TRUE, scale = TRUE)) 


str(scalediris)

set.seed(1000)
#We also need to create test and train data sets, we will do this slightly differently by using the sample function. The 2 says create 2 data sets essentially, replacement means we can reset the random sampling across each vector and the probability gives sample the weight of the splits, 2/3 for train, 1/3 for test. 
iris_sample <- sample(2, nrow(scalediris), replace=TRUE, prob=c(0.67, 0.33))
#We then just need to use the new variable to create the test/train outputs, selecting the first four rows as they are the numeric data in the iris data set and we want to predict Species (https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/sample)

View(iris)



View(iris_sample)

iris_training <- scalediris[iris_sample==1, 1:4]
iris_test <- scalediris[iris_sample==2, 1:4]
#Now we need to create our 'Y' variables or labels need to input into the KNN function
iris.trainLabels <- iris[iris_sample==1, 5]
iris.testLabels <- iris[iris_sample==2, 5]
#So now we will deploy our model 

iris_pred <- knn(train = iris_training, test = iris_test, cl=iris.trainLabels, k=3, prob = TRUE)

View(iris_pred)

View(attr(iris_pred, "prob"))

library(gmodels)
IRISPREDCross <- CrossTable(iris.testLabels, iris_pred, prop.chisq = FALSE)
#Looks like we got all but three correct, not bad


#You can also use caret for KNN, but it's not as specialized as the above, but but does have some additional capabilities for evaluation. 
```
