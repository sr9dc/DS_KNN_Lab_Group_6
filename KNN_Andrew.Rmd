---
title: "KNN Lab"
author: "Andrew Porter"
date: "4/4/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# install libraries
library('tidyverse')
library('caret')
library('class')
library(help = "class")
library("dplyr")
#install.packages('e1071', dependencies=TRUE)

```

```{r}
# read the cvs and import the data
cnn_labels = read_csv("cnn_commmercial_label.csv", col_names = FALSE)
TV_data = read_csv("tv_commercial_datasets_CNN_Cleaned.csv")

# put labels onto tv data
colnames(TV_data)<-t(cnn_labels)
```

```{r}
#2. Determine the split between commercials and non commercials and calculate the base rate. Assume 1 is the commercial label and -1 is the non commercial label

# Get the percentages
pct_comm <- c((nrow(filter(TV_data, label == 1)) / nrow(TV_data)))
pct_non_comm <- c((nrow(filter(TV_data, label == -1)) / nrow(TV_data)))

#Convert into dataframe
TV_pcts = data.frame(
"Commercial percentage" = pct_comm,
"Non Commercial percentage" = pct_non_comm
)
TV_pcts

```

```{r}
#3. Since there are columns that contain different metrics for the same variable (i.e. any column that ends in 'mn' is the mean of that variable, while any column that ends in 'var' is the variance of that variable), we dont need to keep both. Drop all the columns that include var.

#Drop var
TV_data.clean_var <- TV_data %>% select(-contains("var"))
TV_data.clean_var

```

```{r}
#4. Before we run kNN, sometimes it's good to check to make sure that our variables are not highly correlated. Use the cor() functionon 'your_dataframe', label it 'commercial_correlations', and view the data, because remember kNN doesnt work well in high deminsions.

commercial_correlations <- cor(TV_data.clean_var)
commercial_correlations
```

```{r}
#5. Determine which variables to remove, high correlations start around .7 or below -.7. I would especially remove variables that appear to be correlated with more than one variable. List your rationale here:

# USing the rationale listed above, we removed the correlations that are above an absolute value of 0.7. This process also cleared groups such as motion_distr_mn which was highly correlated with multiple variables including frame_diff_dist_mn and motion_dist_mn

TV_data.clean_corr = findCorrelation(commercial_correlations, cutoff = 0.7)
TV_data.clean_corr = sort(TV_data.clean_corr)
TV_data.cleaned = TV_data.clean_var[,-c(TV_data.clean_corr)]

# view the cleaned data and its correlation
TV_data.cleaned
cor(TV_data.cleaned)
```

```{r}
#6. Use the index to generate a train and test sets, then check the row counts to be safe.

set.seed(1982)
TV_data_train_rows = sample(1:nrow(TV_data.cleaned),#<- from 1 to the number of rows in the data set
                              round(0.8 * nrow(TV_data.cleaned), 0),  #<- multiply the number of rows by 0.8 and round the decimals
                              replace = FALSE)#<- don't replace the numbers
head(TV_data_train_rows)

# Make sure 80% of the rows are accounted for.

length(TV_data_train_rows) / nrow(TV_data.cleaned)

# Select the rows identified in the bank_data_train_rows data
TV_data_train = TV_data.cleaned[TV_data_train_rows, ] 

# Select the rows not identified in the bank_data_train_rows data
TV_data_test = TV_data.cleaned[-TV_data_train_rows, ] 

#Check the rows in each set
nrow(TV_data_train)
nrow(TV_data_test)

```

```{r}
#7. Train the classifierusing k = 3, remember to set.seed so you can repeat the output and to use the labels as a vector for the class (not a index of the dataframe)

# k-Nearest Neighbor is a randomized algorithm, so make sure to use set.seed() to make your results repeatable.
set.seed(1982)

# train set cases
TV_3NN <-  knn(train = TV_data_train[, colnames(TV_data.cleaned)],
# training set cases
               test = TV_data_test[, colnames(TV_data.cleaned)],
# category for true classification
               cl = TV_data_train[, "label", drop=TRUE],
# number of neighbors considered
               k = 3,
# control ties between class assignments If true, all distances equal to the kth largest are included
               use.all = TRUE,
               prob = TRUE) 

# View the output.
str(TV_3NN)
length(TV_3NN)
table(TV_3NN)
attributes(TV_3NN)

prb <- data.frame(prob=attr(TV_3NN, "prob"))
(prb)

```

```{r}
#8 Create a initial confusion matrix using the table function and pass it to a object. (xx <- your confusion matrix)

# How does the kNN classification compare to the true class? Let's take a look at the confusion matrix by combining the predictions from bank_3NN to the original data set.
kNN_res = table(TV_3NN, TV_data_test$label)
# Initial Confusion Matrix
kNN_res 
# Select the true positives and true negatives by selecting
# only the cells where the row and column names are the same.
kNN_res[row(kNN_res) == col(kNN_res)]
# Calculate the accuracy rate by dividing the correct classifications
# by the total number of classifications.
kNN_acc <-  sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)
kNN_sen <- kNN_res[2,2]/(kNN_res[2,2]+kNN_res[1,2])
kNN_sen
x <- (kNN_res[1,2])
kNN_acc
```

```{r}
#9  Run the confusion matrix function and comment on the model output

confusionMatrix(as.factor(TV_3NN), as.factor(TV_data_test$label), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

# Accuracy = 76.76%, Sensitivity = 86.73%, Specificity = 59%. Based on Mr. Rooney's goal, we should choose a specificity that is low. 
```

```{r}
#10 Run the "chooseK" function to find the perfect K, while using sapply() function on chooseK() to test k from 1 to 21 (only selecting the odd numbers), and set the train_set argument to 'commercial_train', val_set to 'commercial_test', train_class to the "label"   column of 'commercial_train', and val_class to the "label" column of 'commercial_test'. Label this  "knn_diff_k_com"


# How does "k" affect classification accuracy? Let's create a function? To calculate classification accuracy based on the number of "k."
chooseK = function(k, train_set, val_set, train_class, val_class){
  
# Build kNN with k neighbors considered.
  set.seed(1)
# training set cases
  class_knn = knn(train = train_set,
# test set cases
                  test = val_set,
# category for classification
                  cl = train_class,
# number of neighbors considered
                  k = k,  
# control ties between class assignments. If true, all distances equal to the kth largest are included
                  use.all = TRUE)
  conf_mat = table(class_knn, val_class)
  
# Calculate the accuracy#could change this to Sensitivity 
  accu = sum(conf_mat[row(conf_mat) == col(conf_mat)]) / sum(conf_mat)                         
  cbind(k = k, accuracy = accu)
}
# The sapply() function plugs in several values into our chooseK function. sapply(x, fun...) "fun" here is passing a function to our k-function. function(x)[function] allows you to apply a series of numbers to a function without running a for() loop! Returns a matrix.

# set k to be odd number from 1 to 21
knn_different_k = sapply(seq(1, 21, by = 2),
                         function(x) chooseK(x, 
                                             train_set = TV_data_train[, colnames(TV_data.cleaned)],
                                             val_set = TV_data_test[, colnames(TV_data.cleaned)],
                                             train_class = TV_data_train[, "label", drop=TRUE],
                                             val_class = TV_data_test[, "label", drop=TRUE]))
knn_different_k
```

```{r}
#11 Create a dataframe so we can visualize the difference in accuracy based on K, convert the matrix to a dataframe

# Reformatting the results to graph
knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])
knn_different_k
```

```{r}
#12 Use ggplot to show the output and comment on the k to select.
ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)

# We should select K=7 or K=17 as they have the highest accuracy
```

```{r}
#13 Rerun the model  with the k you selected, assuming it's different. 
TV_7NN <-  knn(train = TV_data_train[, colnames(TV_data.cleaned)],
               test = TV_data_test[, colnames(TV_data.cleaned)],
               cl = TV_data_train[, "label", drop=TRUE],
               k = 7,
               use.all = TRUE,
               prob = TRUE)
```

```{r}
#14 Use the confusion matrix function to measure the quality of the new model.

confusionMatrix(as.factor(TV_7NN), as.factor(TV_data_test$label), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")
```

```{r}
#15 Summarize the differences in language Mr. Rooney may actually understand. Include a discussion on which approach k=3 or k="optimal" is the better method moving forward for "MEH". Most importantly draft comments about the overall approach and model quality as it relates to addressing the problem proposed by Ed. 
```
The results found that the k=3 method had an accuracy of 76.76%, and a sensitivity of 86.73%. The k=7 method on the other hand had an accuracy of 77.87% and a sensitivity of 89.26%.

Our base rate was about 63.9%. 

Since "MEH" wants to make commercials that are more like TV shows, it would be beneficial to observe when a commercial is classified as a false negative- basically when a TV clip is said not to be a commercial when it actually is. For Mr. Rooney, a k=7 (optimal approach) compared to a k=3 approach might help out with diagnosing true positives since the sensitivity increases- when a TV clip is said to be a commercial and it actually is. 

